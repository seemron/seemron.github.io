---
layout: base.njk
pageTitle: "Paper Summaries | Seemron Neupane"
permalink: /paper_summaries/
extraScripts: '<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>'
---
<div class="container pt-5">
    <div class="row justify-content-center">
        <div class="col-lg-8">
            <p class="aligned-text">
                Below are a subset of the papers that I scribbled notes for on a whim.
                Some of these are incomplete, and others may be missing parts.
                Mainly here to keep myself accountable for reading papers. :)
            </p>

            <div class="paper-titles">Example Paper</div>
            <details>
                <summary>
                    Attention Is All You Need (Vaswani et al., 2017)
                </summary>
                <div class="content">
                    <p>
                        Introduces the Transformer architecture, replacing recurrence and convolutions with self-attention.
                        This improves parallelization and enables strong sequence modeling performance.
                    </p>

                    <b>Method</b>
                    <p>
                        Uses stacked encoder and decoder blocks with multi-head self-attention, feed-forward layers,
                        residual connections, layer normalization, and positional encodings.
                    </p>

                    <b>Results</b>
                    <p>
                        Achieves state-of-the-art translation quality at the time while training significantly faster
                        than prior recurrent models.
                    </p>
                </div>
            </details>
        </div>
    </div>
</div>
